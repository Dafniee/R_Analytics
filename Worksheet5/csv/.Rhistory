geom_bar(stat = "identity", position = "dodge") +
labs(title = "Average Product Price by Category Over Time",
x = "Month",
y = "Average Price (USD)") +
theme_minimal() +
scale_fill_brewer(palette = "Set3")  # Use a color palette for categ
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Simulated scraped data (replace with actual data after scraping)
set.seed(123)
category <- rep(c("Books", "Electronics", "Fashion", "Home & Kitchen", "Sports & Outdoors"), each = 6)
month <- rep(1:6, times = 5)
prices <- round(runif(30, min = 10, max = 100), 2)  # Simulated prices
ratings <- round(runif(30, min = 1, max = 5), 2)   # Simulated ratings (1 to 5 stars)
# Create a data frame
data <- data.frame(
Category = category,
Month = month,
Price = prices,
Rating = ratings
)
# 1. Calculate average price and rating by category
avg_data <- data %>%
group_by(Category) %>%
summarize(Average_Price = mean(Price),
Average_Rating = mean(Rating))
# 2. Basic Plot: Price by Category
# Using Base R Plot
barplot(avg_data$Average_Price,
names.arg = avg_data$Category,
col = "skyblue",
main = "Average Product Price by Category",
xlab = "Category",
ylab = "Average Price (USD)",
border = "white")
# 3. ggplot2 Bar Plot for Price
ggplot(avg_data, aes(x = Category, y = Average_Price, fill = Category)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(title = "Average Product Price by Category",
x = "Category",
y = "Average Price (USD)") +
theme_minimal() +
scale_fill_brewer(palette = "Set3")
# 4. Basic Plot: Rating by Category
# Using Base R Plot
barplot(avg_data$Average_Rating,
names.arg = avg_data$Category,
col = "lightgreen",
main = "Average Product Rating by Category",
xlab = "Category",
ylab = "Average Rating (1 to 5)",
border = "white")
# 5. ggplot2 Bar Plot for Rating
ggplot(avg_data, aes(x = Category, y = Average_Rating, fill = Category)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(title = "Average Product Rating by Category",
x = "Category",
y = "Average Rating (1 to 5)") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
# Load necessary library
library(dplyr)
# Simulate data (replace with actual scraped data)
set.seed(123)
category <- rep(c("Books", "Electronics", "Fashion", "Home & Kitchen", "Sports & Outdoors"), each = 6)
month <- rep(1:6, times = 5)
prices <- round(runif(30, min = 10, max = 100), 2)
ratings <- round(runif(30, min = 3, max = 5), 1)  # Ratings between 3 and 5
# Create a data frame
data <- data.frame(
Category = category,
Month = month,
Price = prices,
Rating = ratings
)
# Rank by Price (descending order)
data_ranked_by_price <- data %>%
group_by(Category) %>%
mutate(Price_Rank = rank(-Price))  # Negative sign for descending order
# Rank by Rating (descending order)
data_ranked_by_rating <- data %>%
group_by(Category) %>%
mutate(Rating_Rank = rank(-Rating))  # Negative sign for descending order
# View the ranked data by price
print(data_ranked_by_price)
# View the ranked data by rating
print(data_ranked_by_rating)
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(polite)
library(httr)
library(rvest)
library(dplyr)
# Load required libraries
library(polite)
library(httr)
library(rvest)
library(dplyr)
# Define URL
imdb_url <- "https://www.imdb.com/chart/toptv/?sort=rank%2Casc"
# Create a polite session
imdb_session <- bow(imdb_url, user_agent = "Educational")
# Scrape Titles
raw_titles <- read_html(imdb_url) %>%
html_nodes('.ipc-title__text') %>%
html_text()
# Load required libraries
library(polite)
library(httr)
library(rvest)
library(dplyr)
# Define URL
imdb_url <- "https://www.imdb.com/chart/toptv/?sort=rank%2Casc"
# Create a polite session
imdb_session <- bow(imdb_url, user_agent = "Educational")
# Scrape Titles
raw_titles <- read_html(imdb_url) %>%
html_nodes('.ipc-title__text') %>%
html_text()
# Clean Titles
title_data <- as.data.frame(raw_titles[3:27], stringsAsFactors = FALSE) # Adjust range if needed
colnames(title_data) <- "ranked_titles"
# Split Titles into Rank and Title
split_titles <- strsplit(as.character(title_data$ranked_titles), "\\.", fixed = FALSE)
title_df <- data.frame(do.call(rbind, split_titles), stringsAsFactors = FALSE)
colnames(title_df) <- c("rank", "title")
title_df <- title_df %>% select(rank, title)
title_df$title <- trimws(title_df$title)
# Store Ranked Titles Data
rank_title_data <- title_df
# Scrape Ratings
rating_data <- read_html(imdb_url) %>%
html_nodes('.ipc-rating-star--rating') %>%
html_text()
# Scrape Votes
vote_data <- read_html(imdb_url) %>%
html_nodes('.ipc-rating-star--voteCount') %>%
html_text()
cleaned_votes <- gsub('[()]', '', vote_data)
# Scrape Episode Counts
episode_data <- read_html(imdb_url) %>%
html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item:nth-of-type(2)') %>%
html_text()
cleaned_episodes <- gsub('[eps]', '', episode_data)
episode_counts <- as.numeric(cleaned_episodes)
# Scrape Release Years
release_years <- read_html(imdb_url) %>%
html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item:nth-of-type(1)') %>%
html_text()
# Debugging: Ensure all components have at least 25 rows
print(length(rank_title_data$title))
print(length(rank_title_data$rank))
print(length(rating_data))
print(length(cleaned_votes))
print(length(episode_counts))
print(length(release_years))
# Trim All Components to Top 25
rank_title_data <- rank_title_data[1:25, ]
rating_data <- rating_data[1:25]
cleaned_votes <- cleaned_votes[1:25]
episode_counts <- episode_counts[1:25]
release_years <- release_years[1:25]
# Create Final Dataframe
tv_shows_data <- data.frame(
Title = rank_title_data$title,
Rank = rank_title_data$rank,
Rating = rating_data,
Voters = cleaned_votes,
Episodes = episode_counts,
Year = release_years,
stringsAsFactors = FALSE
)
# Print Final Dataframe (25 rows x 6 columns)
print(tv_shows_data)
# Define the main URL
main_url <- 'https://www.imdb.com/chart/toptv/'
main_page_html <- read_html(main_url)
# Extract links for individual shows
show_links <- main_page_html %>%
html_nodes("td.titleColumn a") %>% # Selector for show links
html_attr("href")
# Fetch detailed data for each show
detailed_show_data <- lapply(show_links[1:50], function(relative_link) { # Process top 50
full_show_link <- paste0("https://www.imdb.com/", relative_link)
# Initialize data fields
user_review_count <- NA
critic_data <- NA
popularity_score <- NA
# Read HTML for each show page
tryCatch({
show_html <- read_html(full_show_link)
# Extract critic reviews
critic_reviews <- show_html %>%
html_nodes('span.score') %>%
html_text()
if (length(critic_reviews) > 1) {
critic_data <- critic_reviews[2]
}
# Extract popularity score
popularity_score <- show_html %>%
html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
html_text()
# Extract user reviews count
review_link <- show_html %>%
html_nodes('a[data-testid="reviews-header"]') %>%
html_attr("href")
if (length(review_link) > 0) {
review_html <- read_html(paste0("https://www.imdb.com", review_link[1]))
user_review_count <- review_html %>%
html_nodes('[data-testid="tturv-total-reviews"]') %>%
html_text()
}
}, error = function(e) {
# Log errors and continue with NA values
message(sprintf("Error processing: %s", full_show_link))
})
# Return detailed show data as a data frame
return(data.frame(
Show_Link = full_show_link,
User_Reviews = user_review_count,
Critic_Reviews = critic_data,
Popularity_Rating = popularity_score,
stringsAsFactors = FALSE
))
})
# Combine detailed data into a single data frame
detailed_show_df <- do.call(rbind, detailed_show_data)
# Combine with the main TV shows data (ensure tv_shows_data has 50 rows)
final_tv_show_data <- cbind(tv_shows_data, detailed_show_df)
# Print the final combined data frame
print(final_tv_show_data)
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(dplyr)
library(rvest)
library(stringr)
# Step 1: Scrape IMDb Top TV Shows (Ranked List)
url <- "https://www.imdb.com/chart/toptv/?sort=rank"
page <- read_html(url)
# Extract the TV Show titles and their ranks
ranked_titles <- page %>%
html_nodes(".titleColumn a") %>%
html_text()
# Convert titles into a data frame and split into rank and title
title_data <- as.data.frame(ranked_titles[3:27], stringsAsFactors = FALSE)
colnames(title_data) <- "ranked_titles"
# Split titles by period to get rank and title
split_titles <- strsplit(as.character(title_data$ranked_titles), "\\.", fixed = FALSE)
# Ensure the split result has two columns for rank and title
title_df <- do.call(rbind, split_titles)
# Check if the split result has 2 columns
if (ncol(title_df) == 2) {
colnames(title_df) <- c("rank", "title")
} else {
# If the split is incorrect, handle it
title_df <- data.frame(rank = rep(NA, length(split_titles)),
title = as.character(split_titles))
}
# Clean up the title column (remove extra spaces)
title_df$title <- trimws(title_df$title)
#save as csv
write.csv(title_df, file = "movie_titles.csv", row.names=FALSE)
# Step 2: Scrape Reviews for Each TV Show
# List of IMDb review page links based on the extracted TV show titles
tv_show_links <- paste0("https://www.imdb.com/title/",
c("tt0903747", "tt5491994", "tt0795176", "tt0185906", "tt7366338"),
"/reviews/?ref_=tt_ov_urv")
# Function to scrape reviews from one IMDb page
scrape_reviews <- function(link, desired_rows = 20) {
page <- read_html(link)
# Extract review details
name <- page %>% html_nodes(".ipc-link.ipc-link--base") %>% html_text()
year <- page %>% html_nodes(".ipc-inline-list__item.review-date") %>% html_text()
rating <- page %>% html_nodes(".ipc-rating-star--rating") %>% html_text()
title <- page %>% html_nodes(".ipc-title__text") %>% html_text()
helpful <- page %>% html_nodes(".ipc-voting__label__count.ipc-voting__label__count--up") %>% html_text()
unhelpful <- page %>% html_nodes(".ipc-voting__label__count.ipc-voting__label__count--down") %>% html_text()
text <- page %>% html_nodes(".ipc-html-content-inner-div") %>% html_text()
# Adjust lengths of vectors by padding shorter ones with NA
name <- c(name, rep(NA, max(0, desired_rows - length(name))))
year <- c(year, rep(NA, max(0, desired_rows - length(year))))
rating <- c(rating, rep(NA, max(0, desired_rows - length(rating))))
title <- c(title, rep(NA, max(0, desired_rows - length(title))))
helpful <- c(helpful, rep(NA, max(0, desired_rows - length(helpful))))
unhelpful <- c(unhelpful, rep(NA, max(0, desired_rows - length(unhelpful))))
text <- c(text, rep(NA, max(0, desired_rows - length(text))))
#cleaning the reviewers named "Permalink"
name <- gsub("Permalink", "ANONYMOUS", name)
name <- str_trim(name)
# Adjust lengths of vectors by padding shorter ones with NA
name <- c(name, rep(NA, max(0, desired_rows - length(name))))
year <- c(year, rep(NA, max(0, desired_rows - length(year))))
rating <- c(rating, rep(NA, max(0, desired_rows - length(rating))))
title <- c(title, rep(NA, max(0, desired_rows - length(title))))
helpful <- c(helpful, rep(NA, max(0, desired_rows - length(helpful))))
unhelpful <- c(unhelpful, rep(NA, max(0, desired_rows - length(unhelpful))))
text <- c(text, rep(NA, max(0, desired_rows - length(text))))
# Truncate vectors if they exceed the desired number of rows
name <- name[1:desired_rows]
year <- year[1:desired_rows]
rating <- rating[1:desired_rows]
title <- title[1:desired_rows]
helpful <- helpful[1:desired_rows]
unhelpful <- unhelpful[1:desired_rows]
text <- text[1:desired_rows]
# Create a data frame
reviews <- data.frame(
name = name,
year = year,
rating = rating,
title = title,
helpful = helpful,
unhelpful = unhelpful,
text = text,
stringsAsFactors = FALSE
)
return(reviews)
}
tv_show_links <- paste0("https://www.imdb.com/title/",
c("tt0903747", "tt5491994", "tt0795176", "tt0185906", "tt7366338"),
"/reviews/?ref_=tt_ov_urv")
all_reviews <- lapply(tv_show_links, scrape_reviews, desired_rows = 20)
combined_reviews <- do.call(rbind, all_reviews)
print(combined_reviews)
#save as csv
write.csv(combined_reviews, file = "movie_reviews.csv")
# Step 3: Scrape Data and Combine
# Loop through all links and scrape data
all_reviews <- lapply(tv_show_links, function(link) scrape_reviews(link, 20))
# Combine data frames into one
final_reviews <- bind_rows(all_reviews, .id = "tv_show_id")
# Add TV show titles and ranks to the reviews
final_reviews <- final_reviews %>%
mutate(tv_show_title = title_df$title[as.integer(tv_show_id)],
rank = title_df$rank[as.integer(tv_show_id)])
# Add TV show titles and ranks to the reviews
final_reviews <- final_reviews %>%
mutate(tv_show_title = title_df$title[as.integer(tv_show_id)],
rank = title_df$rank[as.integer(tv_show_id)])
#Remove specified columns
final_reviews <- final_reviews %>%
select(-helpful, -unhelpful, -tv_show_title, -rank)
# View the modified dataframe
print(final_reviews)
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(polite)
library(httr)
library(rvest)
library(dplyr)
# Extract links for individual shows
show_links <- main_page_html %>%
html_nodes("td.titleColumn a") %>% # Selector for show links
html_attr("href")
# Fetch detailed data for each show
detailed_show_data <- lapply(show_links[1:50], function(relative_link) { # Process top 50
full_show_link <- paste0("https://www.imdb.com/", relative_link)
# Initialize data fields
user_review_count <- NA
critic_data <- NA
popularity_score <- NA
# Read HTML for each show page
tryCatch({
show_html <- read_html(full_show_link)
# Extract critic reviews
critic_reviews <- show_html %>%
html_nodes('span.score') %>%
html_text()
if (length(critic_reviews) > 1) {
critic_data <- critic_reviews[2]
}
# Extract popularity score
popularity_score <- show_html %>%
html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
html_text()
# Extract user reviews count
review_link <- show_html %>%
html_nodes('a[data-testid="reviews-header"]') %>%
html_attr("href")
if (length(review_link) > 0) {
review_html <- read_html(paste0("https://www.imdb.com", review_link[1]))
user_review_count <- review_html %>%
html_nodes('[data-testid="tturv-total-reviews"]') %>%
html_text()
}
}, error = function(e) {
# Log errors and continue with NA values
message(sprintf("Error processing: %s", full_show_link))
})
# Return detailed show data as a data frame
return(data.frame(
Show_Link = full_show_link,
User_Reviews = user_review_count,
Critic_Reviews = critic_data,
Popularity_Rating = popularity_score,
stringsAsFactors = FALSE
))
})
# Combine detailed data into a single data frame
detailed_show_df <- do.call(rbind, detailed_show_data)
# Combine with the main TV shows data (ensure tv_shows_data has 50 rows)
final_tv_show_data <- cbind(tv_shows_data, detailed_show_df)
# Print the final combined data frame
print(final_tv_show_data)
knitr::opts_chunk$set(echo = TRUE)
# View the modified dataframe
print(final_reviews)
library(dplyr)
library(ggplot2)
library(rvest)
# Scrape IMDb data
link = "https://www.imdb.com/title/tt0903747/reviews/?ref_=tt_ov_urv"
page = read_html(link)
# Extract relevant data
years_raw = page %>% html_nodes(".ipc-inline-list__item.review-date") %>% html_text()
# Clean and parse the years
years <- gsub(".* ([0-9]{4})", "\\1", years_raw) # Extract only the year
years <- as.numeric(years) # Convert to numeric
# Summarize the data
year_count <- data.frame(table(years)) %>%
rename(Year = years, Count = Freq) %>%
arrange(Year)
# Plot the time series graph
ggplot(data = year_count, aes(x = Year, y = Count)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "Number of TV Shows Released by Year", x = "Year", y = "Count") +
theme_minimal()
# Find the year with the most TV shows
most_shows <- year_count[which.max(year_count$Count), ]
print(paste("Year with the most TV shows released:", most_shows$Year, "with", most_shows$Count, "TV shows."))
library(polite)
library(httr)
library(rvest)
library(dplyr)
library(magrittr)
library(dplyr)
ranked_by_price <- amazon_data %>%
group_by(Category) %>%
arrange(desc(Price), .by_group = TRUE) %>%
mutate(Price_Rank = row_number())
polite::use_manners(save_as = 'polite_scrape.R')
url1 <- 'https://www.amazon.com/s?k=kitchen&_encoding=UTF8&content-id=amzn1.sym.5b5b28b3-a1ac-480e-bf55-5b98e8879363&pd_rd_r=2637c074-4dee-4944-aa79-6de58b3bb960&pd_rd_w=RAKi0&pd_rd_wg=lnkJ6&pf_rd_p=5b5b28b3-a1ac-480e-bf55-5b98e8879363&pf_rd_r=YJP1GMW8QWWDW2YW65K7&ref=pd_hp_d_btf_unk'
session <- bow(url1, user_agent = "Educational")
page1 <- read_html(url1)
Product_name1 <- page1 %>%
html_nodes('h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-4') %>%
html_text() %>% head(30)
Price1 <- page1 %>%
html_nodes("span.a-price span.a-offscreen") %>%
html_text() %>% head(30)
Rating1 <- page1 %>%
html_nodes("div.a-row.a-size-small") %>%
html_text() %>% head(30)
Description1 <- page1 %>%
html_nodes("h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-4") %>%
html_text() %>% head(30)
Kitchen <- data.frame(
Product = Product_name1,
Price = Price1,
Rating = Rating1,
Description = Description1
)
url1 <- 'https://www.amazon.com/s?k=kitchen&_encoding=UTF8&content-id=amzn1.sym.5b5b28b3-a1ac-480e-bf55-5b98e8879363&pd_rd_r=2637c074-4dee-4944-aa79-6de58b3bb960&pd_rd_w=RAKi0&pd_rd_wg=lnkJ6&pf_rd_p=5b5b28b3-a1ac-480e-bf55-5b98e8879363&pf_rd_r=YJP1GMW8QWWDW2YW65K7&ref=pd_hp_d_btf_unk'
session <- bow(url1, user_agent = "Educational")
page1 <- read_html(url1)
Product_name1 <- page1 %>%
html_nodes('h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-4') %>%
html_text() %>% head(30)
Price1 <- page1 %>%
html_nodes("span.a-price span.a-offscreen") %>%
html_text() %>% head(30)
Rating1 <- page1 %>%
html_nodes("div.a-row.a-size-small") %>%
html_text() %>% head(30)
Description1 <- page1 %>%
html_nodes("h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-4") %>%
html_text() %>% head(30)
Kitchen <- data.frame(
Product = Product_name1,
Price = Price1,
Rating = Rating1,
Description = Description1
)
url2 <- 'https://www.amazon.com/s?k=suitcases&_encoding=UTF8&content-id=amzn1.sym.1faf5a75-f10d-481a-9299-d0fe2e7649bd&pd_rd_r=b0d805dd-e394-40f0-a3f7-d33e50f1683c&pd_rd_w=glz5T&pd_rd_wg=qjjMx&pf_rd_p=1faf5a75-f10d-481a-9299-d0fe2e7649bd&pf_rd_r=MBXMQD5DHEWPYZDXWX0D&ref=pd_hp_d_btf_unk'
session <- bow(url2, user_agent = "Educational")
page2 <- read_html(url2)
Product_name2 <- page2 %>%
html_nodes("span.a-size-base-plus.a-color-base.a-text-normal") %>%
html_text() %>% head(30)
Price2 <- page2 %>%
html_nodes("span.a-price-whole") %>%
html_text() %>% head(30)
Rating2 <- page2 %>%
html_nodes("div.a-section.a-spacing-none.a-spacing-top-micro") %>%
html_text() %>% head(30)
Description2 <- page2 %>%
html_nodes("span.a-size-base-plus.a-color-base.a-text-normal") %>%
html_text() %>% head(30)
Link2 <- page2 %>%
html_nodes('a.a-link-normal') %>%
html_attr('href') %>%
head(30)
SuitCases <- data.frame(
Product = Product_name2,
Price = Price2,
Rating = Rating2,
Description = Description2,
Link = Link2
)
setwd("~/R Progrraming/Worksheet5")
